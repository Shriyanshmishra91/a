{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCF8kEzEH0MtobYsv1pfgL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriyanshmishra91/a/blob/main/svm_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ":QUES1\n",
        "What is a Support Vector Machine (SVM), and how does it work?\n",
        "ANSWER\n",
        "\n",
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. Its primary goal is to find the optimal hyperplane that best separates different classes of data points in a high-dimensional space.\n",
        "\n",
        "Here's a simplified breakdown of how it works:\n",
        "\n",
        "Finding the Hyperplane: In a 2D space, the hyperplane is a line. In higher dimensions, it's a plane or a hyperplane. The SVM aims to find the hyperplane that maximizes the margin between the different classes. The margin is the distance between the hyperplane and the nearest data points from each class (these points are called support vectors).\n",
        "Maximizing the Margin: By maximizing the margin, the SVM creates a buffer zone around the decision boundary. This makes the model more robust and less sensitive to noise in the data, leading to better generalization on unseen data.\n",
        "Support Vectors: The data points that lie closest to the hyperplane and influence its position are called support vectors. These are the crucial data points that define the margin and the hyperplane. If you remove a support vector, the position of the hyperplane might change.\n",
        "Handling Non-linearly Separable Data: In many real-world scenarios, data is not perfectly separable by a straight line or plane. SVMs can handle this using a technique called the \"kernel trick.\" The kernel trick allows SVMs to implicitly map the data into a higher-dimensional space where it might become linearly separable, without explicitly calculating the coordinates in that higher dimension. Common kernel functions include the linear kernel, polynomial kernel, and radial basis function (RBF) kernel.\n",
        "In essence, SVMs are effective because they focus on the boundary data points (support vectors) and aim to find the best possible separation line (hyperplane) by maximizing the margin between classes, even in complex, non-linearly separable datasets.\n",
        "Ques2\n",
        "\n",
        "explain the difference beEtween Hard Margin and Soft Margin SVM.\n",
        "\n"
      ],
      "metadata": {
        "id": "PesDMCGQdkn6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43b46a97"
      },
      "source": [
        "QUES2explain the difference beEtween Hard Margin and Soft Margin SVM.\n",
        "\n",
        "**Hard Margin SVM:**\n",
        "\n",
        "*   **Strict Separation:** Aims for a perfect separation of the data points into distinct classes. It requires that all data points are on the correct side of the margin and the hyperplane.\n",
        "*   **No Misclassifications Allowed:** In a Hard Margin SVM, there is zero tolerance for misclassification errors. If the data is not perfectly linearly separable, a Hard Margin SVM cannot find a valid hyperplane.\n",
        "*   **Sensitive to Outliers:** Outliers can significantly impact the position of the hyperplane and margin in a Hard Margin SVM, potentially leading to a less optimal separation.\n",
        "*   **Applicable when data is linearly separable and noise-free:** This approach works well when the data is clearly separable by a linear boundary and there are no or very few outliers.\n",
        "\n",
        "**Soft Margin SVM:**\n",
        "\n",
        "*   **Allows for Some Misclassifications:** Unlike Hard Margin SVM, Soft Margin SVM allows for a certain degree of misclassification or points falling within the margin. It introduces a \"slack\" variable that permits some data points to violate the strict margin or even be on the wrong side of the hyperplane.\n",
        "*   **Regularization Parameter (C):** Soft Margin SVM uses a regularization parameter (often denoted as 'C') to control the trade-off between maximizing the margin and minimizing the misclassification errors.\n",
        "    *   A **small C** allows for a wider margin but more misclassifications.\n",
        "    *   A **large C** enforces a stricter margin but is more sensitive to misclassifications.\n",
        "*   **More Robust to Noise and Outliers:** By allowing some errors, Soft Margin SVM is more robust to noisy data and outliers, as they have less influence on the position of the hyperplane.\n",
        "*   **Applicable when data is not perfectly linearly separable or contains noise:** This is a more practical approach for most real-world datasets, which often contain noise or are not perfectly separable.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "| Feature           | Hard Margin SVM                  | Soft Margin SVM                     |\n",
        "|-------------------|---------------------------------|--------------------------------------|\n",
        "| Separation        | Strict (no misclassifications)  | Allows some misclassifications       |\n",
        "| Misclassifications| Not allowed                     | Allowed (controlled by 'C')          |\n",
        "| Sensitivity      | High (to outliers)              | Lower (more robust)                  |\n",
        "| Data Requirements | Linearly separable, noise-free  | Can handle non-separable/noisy data |\n",
        "\n",
        "Soft Margin SVM is a more flexible and commonly used approach in practice because real-world data is rarely perfectly linearly separable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64e86eb"
      },
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "explain its use case.\n",
        "##\" the kernal trick\"\n",
        "The **Kernel Trick** is a powerful technique used in Support Vector Machines (SVMs) that allows them to effectively handle non-linearly separable data without explicitly transforming the data into a higher-dimensional space.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "*   **The Problem:** Linear SVMs work by finding a linear hyperplane to separate data. However, many real-world datasets are not linearly separable in their original dimension. To separate such data, we might need to map it to a higher-dimensional space where a linear separator (a hyperplane) can be found. Explicitly calculating the coordinates in this higher-dimensional space can be computationally expensive or even impossible.\n",
        "*   **The Solution: The Kernel Trick:** The kernel trick avoids the explicit mapping to a higher dimension. Instead, it uses a **kernel function** that calculates the dot product of the data points *as if they were in the higher-dimensional space*, without actually performing the transformation.\n",
        "*   **How it helps:** The SVM algorithm only needs the dot products of the data points to find the optimal hyperplane. By using a kernel function, we can obtain these dot products efficiently, even for very high or infinite dimensional spaces, without the computational cost of the explicit transformation.\n",
        "\n",
        "Essentially, the kernel trick allows SVMs to learn non-linear decision boundaries in the original feature space by implicitly working in a higher-dimensional space.\n",
        "\n",
        "### Example of a Kernel: The Radial Basis Function (RBF) Kernel\n",
        "\n",
        "One of the most commonly used kernel functions is the **Radial Basis Function (RBF) kernel**, also known as the Gaussian kernel.\n",
        "\n",
        "The formula for the RBF kernel between two data points $x_i$ and $x_j$ is:\n",
        "\n",
        "$K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$\n",
        "\n",
        "where:\n",
        "\n",
        "*   $\\|x_i - x_j\\|^2$ is the squared Euclidean distance between $x_i$ and $x_j$.\n",
        "*   $\\gamma$ (gamma) is a parameter that controls the influence of a single training example. A smaller $\\gamma$ means a larger influence, and vice versa.\n",
        "\n",
        "**Use Case of the RBF Kernel:**\n",
        "\n",
        "The RBF kernel is particularly useful when:\n",
        "\n",
        "*   The relationship between the features and the target variable is **non-linear**.\n",
        "*   You don't have prior knowledge about the structure of the data.\n",
        "\n",
        "The RBF kernel can create complex, non-linear decision boundaries, making it suitable for a wide variety of datasets where a linear separation is not possible. For example, it's often used in image recognition, text classification, and other tasks where the data has intricate patterns.\n",
        "\n",
        "In essence, the RBF kernel measures the similarity between two points based on their distance in the original feature space. Points that are closer together in the original space will have a higher kernel value, and this similarity information is used by the SVM to find a non-linear decision boundary in the implicit higher-dimensional space.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uht57mru3mzU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f1e78f"
      },
      "source": [
        "## Naïve Bayes ClassifierQuestion 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "Answer:\n",
        "\n",
        "\n",
        "A **Naïve Bayes Classifier** is a probabilistic machine learning algorithm based on Bayes' Theorem. It's primarily used for classification tasks.\n",
        "\n",
        "Here's the core idea:\n",
        "\n",
        "It calculates the probability of a given data point belonging to a particular class based on the probabilities of its features. It assumes that the presence of a particular feature in a class is independent of the presence of any other feature.\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The term \"naïve\" comes from the **strong (and often unrealistic) assumption of independence** between the features. In many real-world datasets, features are often correlated. For example, in a spam email classifier, the presence of the word \"free\" might be correlated with the presence of the word \"money.\" The Naïve Bayes classifier, however, assumes these features are independent.\n",
        "\n",
        "Despite this \"naïve\" assumption, Naïve Bayes classifiers often perform surprisingly well in practice, especially with large datasets. They are also computationally efficient and easy to implement.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "*   **What it is:** A probabilistic classifier based on Bayes' Theorem.\n",
        "*   **How it works:** Calculates the probability of a data point belonging to a class based on feature probabilities, assuming feature independence.\n",
        "*   **Why \"Naïve\":** Due to the strong assumption that features are independent of each other.\n",
        "*   **Strengths:** Simple, fast, and often effective, especially for text classification and spam filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n"
      ],
      "metadata": {
        "id": "S8KGW6hq3oHz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df038f74"
      },
      "source": [
        "##QUES 5Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one? Naïve Bayes Variants\n",
        "\n",
        "There are several variants of the Naïve Bayes classifier, differing mainly in the assumptions they make about the distribution of the features. The most common ones are:\n",
        "\n",
        "### 1. Gaussian Naïve Bayes\n",
        "\n",
        "*   **Assumption:** Assumes that the features follow a Gaussian (normal) distribution.\n",
        "*   **Use Case:** This variant is typically used when your features are **continuous** numerical values. It calculates the mean and standard deviation of each feature for each class during the training phase and uses these to estimate the probability of a given feature value belonging to a class using the Gaussian probability density function.\n",
        "*   **When to use:** Suitable for datasets where your features are real-valued and can be reasonably assumed to follow a normal distribution (e.g., height, weight, temperature).\n",
        "\n",
        "### 2. Multinomial Naïve Bayes\n",
        "\n",
        "*   **Assumption:** Assumes that the features represent the **counts** or frequencies of events, such as the number of times a word appears in a document. It's based on the multinomial distribution.\n",
        "*   **Use Case:** This variant is widely used for **text classification problems**, where features are often word counts or term frequencies. It works well with discrete features representing counts.\n",
        "*   **When to use:** Ideal for tasks like spam filtering, document categorization, and sentiment analysis, where you are dealing with data that can be represented as frequency counts.\n",
        "\n",
        "### 3. Bernoulli Naïve Bayes\n",
        "\n",
        "*   **Assumption:** Assumes that the features are **binary** (0 or 1), indicating the presence or absence of a particular event. It's based on the Bernoulli distribution.\n",
        "*   **Use Case:** This variant is also suitable for **text classification**, but instead of using word counts, it considers whether a word is present or absent in a document. It's useful when you care more about the presence of a term rather than its frequency.\n",
        "*   **When to use:** Can be used for text classification where a binary representation of features is appropriate, or for any dataset with binary features (e.g., whether a customer clicked on an ad or not).\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "| Variant      | Feature Type      | Assumption                     | Common Use Cases              |\n",
        "|--------------|-------------------|--------------------------------|-------------------------------|\n",
        "| Gaussian     | Continuous        | Gaussian distribution          | Numerical data with normal dist.|\n",
        "| Multinomial  | Discrete (counts) | Multinomial distribution       | Text classification (counts)  |\n",
        "| Bernoulli    | Binary (0/1)      | Bernoulli distribution         | Text classification (presence)|\n",
        "\n",
        "Choosing the right variant depends on the nature of your features and the type of data you are working with."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".\n",
        "When would you use each one?\n",
        "\n"
      ],
      "metadata": {
        "id": "Gv9XrlzH44mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "jpW3dlne5pP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION7#Question 6: Write a Python program to:\n",
        "# Load the Iris dataset\n",
        "#Train an SVM Classifier with a linear kernel\n",
        "\n",
        "\n",
        "#Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "R6Y9q1F38UE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train an SVM classifier with a linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# 6. Print the support vectors\n",
        "support_vectors = svm_linear.support_vectors_\n",
        "print(f\"Support Vectors:\\n {support_vectors}\")\n",
        "\n",
        "# 7. Optional: Print the number of support vectors per class\n",
        "for i in range(len(svm_linear.classes_)):\n",
        "    num_support = np.sum(svm_linear.support_ == i)\n",
        "    print(f\"Number of support vectors for class {i}: {num_support}\")\n",
        "\n",
        "# 8. Optional: Visualize the data and decision boundaries (requires matplotlib)\n",
        "# import matplotlib.pyplot as plt\n",
        "# from mlxtend.plotting import plot_decision_regions\n",
        "#\n",
        "# # Assuming a 2D projection for visualization\n",
        "# X_embedded = X[:, :2]  # Use first two features\n",
        "# X_train_embedded = X_train[:, :2]\n",
        "# X_test_embedded = X_test[:, :2]\n",
        "#\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plot_decision_regions(X_embedded, y, clf=svm_linear, legend=2)\n",
        "# plt.xlabel(\"Sepal Length (cm)\")\n",
        "# plt.ylabel(\"Sepal Width (cm)\")\n",
        "# plt.title(\"SVM Linear Kernel Decision Regions\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zuRDBmU8vC6",
        "outputId": "f0af2422-d55c-4153-deef-8a8496b4a27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "Number of support vectors for class 0: 0\n",
            "Number of support vectors for class 1: 1\n",
            "Number of support vectors for class 2: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTTION8● #Train an SVM Classifier on the Wine dataset using GridSearchCV to find the bEST\n",
        "# Print the best hyperparameters and accuracy.\n",
        "#Include your Python code and output in the code box below\n"
      ],
      "metadata": {
        "id": "O5lHGdJe6a7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fdfd238",
        "outputId": "fc5344f3-0da7-4357-bf8c-32abe9299f4c"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001],\n",
        "              'kernel': ['rbf']} # Using RBF kernel for demonstration with gamma\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the corresponding accuracy\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Evaluate the model with the best parameters on the test set\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy with best parameters: {accuracy:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
            "Best parameters found by GridSearchCV:\n",
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Accuracy with best parameters: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ques9 Write a Python program to:\n",
        "# Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "#sklearn.datasets.fetch_20newsgroups).\n",
        "# Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1JmJOsDY-KnS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04d97f05"
      },
      "source": [
        "# Task\n",
        "Write a Python program to: Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). Print the model's ROC-AUC score for its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9de775f"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load a synthetic text dataset using `sklearn.datasets.fetch_20newsgroups`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7c3534"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to load the 20 newsgroups dataset. I will use the `fetch_20newsgroups` function from `sklearn.datasets` to load a subset of the data and store the data and target in separate variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ff1a36b",
        "outputId": "dada50b3-73de-4f6c-b74d-59b065feddf1"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Select a subset of categories for simplicity\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "\n",
        "# Load the dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "print(f\"Number of training samples: {len(X_train)}\")\n",
        "print(f\"Number of testing samples: {len(X_test)}\")\n",
        "print(f\"Categories: {newsgroups_train.target_names}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 2257\n",
            "Number of testing samples: 1502\n",
            "Categories: ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "979034ad"
      },
      "source": [
        "## Preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Split the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4858c7a7"
      },
      "source": [
        "## Vectorize text data\n",
        "\n",
        "### Subtask:\n",
        "Convert the text data into numerical features that can be used by the Naïve Bayes classifier. Using `TfidfVectorizer` is a common approach for text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b9921dc"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the text data into numerical features using TfidfVectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10:# Imagine you’re working as a data scientist for a company that handles\n",
        "#email communications.\n",
        "#Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "#contain:\n",
        "# Text with diverse vocabulary\n",
        "# Potential class imbalance (far more legitimate emails than spam)\n",
        "# Some incomplete or missing data\n",
        "#explain the approach you would take to:\n",
        "# Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "#Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "# Address class imbalance\n",
        "# Evaluate the performance of your solution with suitable metrics\n",
        "#And explain the business impact of your solution."
      ],
      "metadata": {
        "id": "8hBGz4UF-_za"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data preprocessing\n",
        "\n",
        "# Text Vectorization: Since machines cannot directly process raw text, text data needs to be converted into numerical representations. Techniques like Bag of Words (BoW) or TF-IDF (Term Frequency-Inverse Frequency) can be employed.\n",
        "\n",
        "# *   **Bag of Words (BoW):** Represents text as a collection of word counts, disregarding word order. While simple, it might lose contextual information.\n",
        "# *   **TF-IDF:** Assigns weights to words based on their frequency in a document and rarity across the entire dataset, highlighting words that are informative and less common.\n",
        "\n",
        "# Handling Missing Data: Missing data can negatively impact model performance, potentially leading to bias and inaccurate results. Several strategies exist, including:\n",
        "\n",
        "# *   **Deletion:** Removing rows or columns with missing values. This is a simple approach but can lead to significant data loss if not done carefully.\n",
        "# *   **Imputation:** Replacing missing values with estimated or predicted values.\n",
        "#    *   **Mean/Median/Mode Imputation:** Replacing missing numerical values with the mean, median, or mode of the column.\n",
        "#    *   **K-Nearest Neighbors (KNN) Imputation:** Estimating missing values based on similar data points.\n",
        "#    *   **Regression Imputation:** Predicting missing values using a regression model trained on other features.\n",
        "\n",
        "# 2. Choosing and justifying an appropriate model (SVM vs. Naive Bayes)\n",
        "\n",
        "# *   **Support Vector Machine (SVM):** Effective for handling complex and high-dimensional data, commonly used in text and image classification. SVM aims to find the optimal hyperplane that separates data points into different classes.\n",
        "# *   **Naive Bayes:** Simple and fast, especially effective for tasks involving short texts like tweets. Naive Bayes operates based on Bayes' theorem, assuming independence among features. While this assumption may not always hold true, it often provides impressive performance.\n",
        "\n",
        "# Justification: The choice between SVM and Naive Bayes depends on factors like the dataset size, data complexity, and computational resources available. SVM may be more suitable for larger datasets with intricate patterns, while Naive Bayes could be sufficient for simpler, smaller datasets.\n",
        "\n",
        "# 3. Addressing class imbalance\n",
        "\n",
        "# Class imbalance, where the number of instances in one class significantly outnumbers the other, can lead to biased models and poor predictive performance for the minority class. Addressing this is crucial and can be achieved through techniques such as:\n",
        "\n",
        "# *   **Resampling Techniques:**\n",
        "#    *   **Undersampling:** Randomly removing instances from the majority class to balance the class distribution.\n",
        "#    *   **Oversampling:** Replicating or generating synthetic samples for the minority class. Methods like SMOTE (Synthetic Minority Over-sampling Technique) can be used to generate artificial instances.\n",
        "# *   **Class Weighting:** Assigning different weights to classes based on their distribution, giving higher importance to the minority class during training.\n",
        "# *   **Ensemble Methods:** Combining multiple models to improve overall performance, especially useful with imbalanced data.\n",
        "\n",
        "# 4. Evaluating the performance with suitable metrics and explaining the business impact\n",
        "\n",
        "# Evaluation Metrics: Choosing the right metrics is essential for assessing model performance. For classification tasks, several metrics can be employed:\n",
        "\n",
        "# *   **Accuracy:** Measures the overall correctness of predictions. However, it can be misleading in imbalanced datasets.\n",
        "# *   **Precision:** Measures the proportion of correctly predicted positive results out of all predicted positive results, important for minimizing false positives.\n",
        "# *   **Recall (Sensitivity):** Measures the model's ability to identify all actual positive instances, crucial for minimizing false negatives.\n",
        "# *   **F1-Score:** The harmonic mean of precision and recall, providing a balanced measure of a model's performance, especially useful in cases of class imbalance.\n",
        "# *   **AUC-ROC:** Evaluates the trade-off between sensitivity and specificity, showing the model's ability to distinguish between positive and negative classes.\n",
        "\n",
        "# Business Impact: The model's performance metrics directly relate to business objectives. For instance:\n",
        "\n",
        "# *   **Increased Accuracy:** A more accurate model in fraud detection can lead to reduced financial losses.\n",
        "# *   **Enhanced Customer Experience:** A model that accurately predicts customer behavior can help personalize recommendations, leading to higher customer satisfaction and retention.\n",
        "# *   **Improved Efficiency:** Automating tasks through machine learning can reduce human error and optimize operational processes, leading to cost savings and increased productivity.\n",
        "\n",
        "# In conclusion, by strategically applying these steps – from careful data preprocessing to appropriate model selection and evaluation, especially when dealing with imbalanced datasets – a robust machine learning solution can be developed that not only performs well but also drives positive business outcomes."
      ],
      "metadata": {
        "id": "RLkEMp-0GTGV"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}