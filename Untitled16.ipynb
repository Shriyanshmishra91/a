{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzKuTvT9aXNo56L53X/WIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriyanshmishra91/a/blob/main/Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWpdavmq8Yb4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer\n",
        "Ensemble Learning is a machine learning technique where multiple models, often referred to as \"weak learners,\" are combined to solve a particular problem. The key idea behind it is that by aggregating the predictions or decisions of several diverse models, the overall performance and robustness of the system can be improved compared to using a single model. This is based on the principle that while individual models might have biases or make errors, combining their insights can lead to a more accurate and reliable outcome.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "Bagging and Boosting are both ensemble learning techniques, but they differ in how they combine the models and how they handle errors.\n",
        "\n",
        "Bagging (Bootstrap Aggregating): In Bagging, multiple models are trained independently on different random subsets of the training data (created by sampling with replacement). The final prediction is typically the average (for regression) or majority vote (for classification) of the individual model predictions. Bagging aims to reduce variance by averaging out the errors of individual models.\n",
        "Boosting: In Boosting, models are trained sequentially. Each new model is trained to correct the errors made by the previous models. It focuses on the data points that were misclassified or had larger errors in the previous iterations. Boosting aims to reduce bias and can achieve higher accuracy than Bagging, but it can also be more\n",
        "\n",
        " Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Answer:Bootstrap sampling is a resampling technique where you create multiple subsets of the original dataset by randomly sampling with replacement. This means that each subset can contain duplicate instances from the original dataset, and some instances from the original dataset may not be included in a particular subset.\n",
        "\n",
        "\n",
        "\n",
        "Qion 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "Answer:Out-of-Bag (OOB) samples are the data points from the original training set that were not included in the bootstrap sample used to train a particular tree in a Bagging ensemble (like a Random Forest). Because bootstrap sampling is done with replacement, each bootstrap sample will typically leave out about one-third of the original data points. These left-out data points form the OOB sample for that specific tree.\n",
        "\n",
        "The OOB score is a way to evaluate the performance of a Bagging ensemble without needing a separate validation set. Here's how it's used:\n",
        "\n",
        "For each data point in the original training set, identify the trees in the ensemble for which this data point was part of the OOB sample.\n",
        "Use these trees to make a prediction for that data point.\n",
        "Aggregate these predictions (e.g., by averaging or taking a majority vote) to get an OOB prediction for that data point.\n",
        "Compare the OOB prediction with the actual target value for that data point.\n",
        "Calculate an overall OOB score (e.g., accuracy for classification, mean squared error for regression) by averaging the performance across all data points in the training set.\n",
        "The OOB score provides an internal estimate of the model's performance on unseen data, similar to cross-validation. It's a convenient way to evaluate Bagging models because it doesn't require splitting the data into explicit training and validation sets, making full use of the available data for training.\n",
        "\n",
        "Would you like me to also answer the last question about comparing feature importance analysis in a single Decision\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "ANSWER:\n",
        "single Decision Tree:\n",
        "\n",
        "Method: Feature importance is typically calculated based on how much each feature reduces the impurity (e.g., Gini impurity or entropy) when it's used to split nodes in the tree. Features that lead to larger impurity reductions are considered more important.\n",
        "Bias: This method can be biased towards features with a large number of unique values, as they are more likely to create splits that significantly reduce impurity, even if their true predictive power is not as high.\n",
        "Stability: The feature importance can be highly sensitive to small changes in the training data, as the structure of a single decision tree can change significantly.\n",
        "Random Forest:\n",
        "\n",
        "Method: Feature importance in a Random Forest is calculated by averaging the feature importances across all the individual decision trees in the forest. This can be done in a couple of ways:\n",
        "Mean Decrease in Impurity: Similar to a single tree, but averaged across all trees.\n",
        "Permutation Importance: This method involves randomly shuffling the values of a feature in the OOB samples and measuring the decrease in model performance (e.g., accuracy or mean squared error). A larger decrease indicates higher feature importance.\n",
        "Bias: Averaging across multiple trees helps to reduce the bias towards features with many unique values that can affect a single tree. Permutation importance is generally considered less biased than impurity-based methods.\n",
        "Stability: Feature importance in a Random Forest is more stable and less sensitive to small data changes because it aggregates information from multiple trees.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xC8U5s-b8d-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION6Write a Python program to:\n",
        "# Load the Breast Cancer dataset using\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "#Train a Random Forest Classifier\n",
        "# Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "T_6Ce3fCAGVD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "02ff43b2",
        "outputId": "9c80b513-bcdd-4ad7-8133-b6d303619538"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = pd.Series(breast_cancer.target)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "feature_importance_series = pd.Series(feature_importances, index=X.columns)\n",
        "\n",
        "# Sort feature importances in descending order and get the top 5\n",
        "top_5_features = feature_importance_series.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "display(top_5_features)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "worst area              0.139357\n",
              "worst concave points    0.132225\n",
              "mean concave points     0.107046\n",
              "worst radius            0.082848\n",
              "worst perimeter         0.080850\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>worst area</th>\n",
              "      <td>0.139357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst concave points</th>\n",
              "      <td>0.132225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean concave points</th>\n",
              "      <td>0.107046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst radius</th>\n",
              "      <td>0.082848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worst perimeter</th>\n",
              "      <td>0.080850</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#qUESTION7\n",
        "#Write a Python program to:\n",
        "# Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "dEgBxIEIBHRd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e28a146e",
        "outputId": "a4627b5c-73e4-4941-a55b-7ff6aaf07daf"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "single_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "single_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the single Decision Tree Classifier\n",
        "single_tree_predictions = single_tree_model.predict(X_test)\n",
        "single_tree_accuracy = accuracy_score(y_test, single_tree_predictions)\n",
        "print(f\"Accuracy of a single Decision Tree: {single_tree_accuracy:.4f}\")\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                                  n_estimators=10,\n",
        "                                  random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the Bagging Classifier\n",
        "bagging_predictions = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "print(f\"Accuracy of Bagging Classifier (with 10 Decision Trees): {bagging_accuracy:.4f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "print(\"\\nComparison:\")\n",
        "if bagging_accuracy > single_tree_accuracy:\n",
        "    print(\"The Bagging Classifier has higher accuracy than the single Decision Tree.\")\n",
        "elif bagging_accuracy < single_tree_accuracy:\n",
        "    print(\"The single Decision Tree has higher accuracy than the Bagging Classifier.\")\n",
        "else:\n",
        "    print(\"Both the Bagging Classifier and the single Decision Tree have the same accuracy.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of a single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier (with 10 Decision Trees): 1.0000\n",
            "\n",
            "Comparison:\n",
            "Both the Bagging Classifier and the single Decision Tree have the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ques8\n",
        "#Write a Python program to:\n",
        "# Train a Random Forest Classifier\n",
        "# Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# Print the best parameters and final accurac"
      ],
      "metadata": {
        "id": "ATC3PRfCBzS7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "dfd69d2c",
        "outputId": "94d02048-9ffe-4faa-cf08-61b5c084bf81"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = pd.Series(breast_cancer.target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'n_estimators': [50, 100, 150, 200]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best Parameters found by GridSearchCV:\")\n",
        "display(grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the final accuracy\n",
        "print(f\"\\nFinal Accuracy with Best Parameters: {final_accuracy:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters found by GridSearchCV:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'max_depth': None, 'n_estimators': 150}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Accuracy with Best Parameters: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UZUtoWjBtsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ec3a76"
      },
      "source": [
        "# Task\n",
        "Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "444e8b50"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the California Housing dataset using `sklearn.datasets.fetch_california_housing()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eff69907"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the California Housing dataset using the specified function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "484e4a6e",
        "outputId": "a8b8ffda-b05b-4c05-8284-95074d43a43b"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "y = pd.Series(california_housing.target)\n",
        "\n",
        "print(\"California Housing dataset loaded successfully.\")\n",
        "display(X.head())\n",
        "display(y.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "California Housing dataset loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
              "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
              "\n",
              "   Longitude  \n",
              "0    -122.23  \n",
              "1    -122.22  \n",
              "2    -122.24  \n",
              "3    -122.25  \n",
              "4    -122.25  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9673a057-9348-4f0b-92d3-53b32529231c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.6431</td>\n",
              "      <td>52.0</td>\n",
              "      <td>5.817352</td>\n",
              "      <td>1.073059</td>\n",
              "      <td>558.0</td>\n",
              "      <td>2.547945</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.8462</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.281853</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.181467</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9673a057-9348-4f0b-92d3-53b32529231c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9673a057-9348-4f0b-92d3-53b32529231c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9673a057-9348-4f0b-92d3-53b32529231c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2b9ac68b-9a48-4f35-a539-b16e6fd73b8d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b9ac68b-9a48-4f35-a539-b16e6fd73b8d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2b9ac68b-9a48-4f35-a539-b16e6fd73b8d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(y\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"MedInc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.9218775476080674,\n        \"min\": 3.8462,\n        \"max\": 8.3252,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8.3014,\n          3.8462,\n          7.2574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HouseAge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.501851724856113,\n        \"min\": 21.0,\n        \"max\": 52.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          41.0,\n          21.0,\n          52.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveRooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9705323807243326,\n        \"min\": 5.8173515981735155,\n        \"max\": 8.288135593220339,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          6.238137082601054,\n          6.281853281853282,\n          8.288135593220339\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveBedrms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04661885487529508,\n        \"min\": 0.9718804920913884,\n        \"max\": 1.0810810810810811,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9718804920913884,\n          1.0810810810810811,\n          1.073446327683616\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Population\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 862.3365352343596,\n        \"min\": 322.0,\n        \"max\": 2401.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2401.0,\n          565.0,\n          496.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveOccup\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2881316535489867,\n        \"min\": 2.109841827768014,\n        \"max\": 2.8022598870056497,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.109841827768014,\n          2.1814671814671813,\n          2.8022598870056497\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0130384048104057,\n        \"min\": 37.85,\n        \"max\": 37.88,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          37.88,\n          37.86,\n          37.85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013038404810404884,\n        \"min\": -122.25,\n        \"max\": -122.22,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -122.22,\n          -122.25,\n          -122.23\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    4.526\n",
              "1    3.585\n",
              "2    3.521\n",
              "3    3.413\n",
              "4    3.422\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.422</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "030e02dd"
      },
      "source": [
        "## Split data\n",
        "\n",
        "### Subtask:\n",
        "Split the dataset into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d26f21d"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data into training and testing sets and print their shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58680fad",
        "outputId": "2c47a211-d49a-4467-f7bb-6773cf07fd70"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting splits\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (16512, 8)\n",
            "Shape of X_test: (4128, 8)\n",
            "Shape of y_train: (16512,)\n",
            "Shape of y_test: (4128,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "768e1914"
      },
      "source": [
        "## Train bagging regressor\n",
        "\n",
        "### Subtask:\n",
        "Train a Bagging Regressor model on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c00474e2"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a Bagging Regressor model on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98e282c5",
        "outputId": "c14d2f40-0a7e-4a0b-916b-63b98d0aa167"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "# Instantiate a BaggingRegressor object\n",
        "bagging_regressor = BaggingRegressor(random_state=42)\n",
        "\n",
        "# Fit the BaggingRegressor model to the training data\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "print(\"Bagging Regressor model trained successfully.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21b95c18"
      },
      "source": [
        "## Train random forest regressor\n",
        "\n",
        "### Subtask:\n",
        "Train a Random Forest Regressor model on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e28a4cc1"
      },
      "source": [
        "**Reasoning**:\n",
        "Train a Random Forest Regressor model on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a86da3",
        "outputId": "dd20ba8e-37e2-40d6-f681-3e0af07733cf"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Instantiate a RandomForestRegressor object\n",
        "random_forest_regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit the RandomForestRegressor model to the training data\n",
        "random_forest_regressor.fit(X_train, y_train)\n",
        "\n",
        "print(\"Random Forest Regressor model trained successfully.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor model trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff2a40e7"
      },
      "source": [
        "## Evaluate models\n",
        "\n",
        "### Subtask:\n",
        "Evaluate both models on the testing data using Mean Squared Error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27a9ff59"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the mean_squared_error function and evaluate both models using MSE on the testing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "179bec65",
        "outputId": "69c77635-ecb5-4447-b602-667b1254d72e"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Make predictions with the Bagging Regressor\n",
        "bagging_predictions = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error for the Bagging Regressor\n",
        "bagging_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "\n",
        "# Make predictions with the Random Forest Regressor\n",
        "random_forest_predictions = random_forest_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error for the Random Forest Regressor\n",
        "random_forest_mse = mean_squared_error(y_test, random_forest_predictions)\n",
        "\n",
        "# Print the MSE for both models\n",
        "print(f\"Mean Squared Error for Bagging Regressor: {bagging_mse:.4f}\")\n",
        "print(f\"Mean Squared Error for Random Forest Regressor: {random_forest_mse:.4f}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error for Bagging Regressor: 0.2824\n",
            "Mean Squared Error for Random Forest Regressor: 0.2554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3063081e"
      },
      "source": [
        "**Reasoning**:\n",
        "Compare the calculated MSE values and print a statement indicating which model performed better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQuestion 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-worldtep 1: Choose Between Bagging and Boosting\n",
        "\n",
        "\n",
        "\n",
        "Consider the nature of errors: Bagging is generally effective at reducing variance and is less prone to overfitting than Boosting. If your base models are likely to have high variance (e.g., deep decision trees), Bagging might be a good starting point. Boosting, on the other hand, focuses on reducing bias and can achieve higher accuracy, but it's more sensitive to noisy data and outliers, and more prone to overfitting if not carefully tuned.\n",
        "Experimentation: The best approach is often to experiment with both techniques. Start with default parameters for each and evaluate their performance on a validation set.\n",
        "Computational Resources: Boosting can be computationally more expensive than Bagging, especially with a large number of estimators. Consider your available resources.\n",
        "Step 2: Handle Overfitting\n",
        "\n",
        "For Bagging:\n",
        "Limit base model complexity: While Bagging can handle complex base models, limiting their depth (for decision trees) or complexity can further reduce variance and prevent overfitting.\n",
        "Increase n_estimators: Using more base models in the ensemble can help average out individual model errors and improve generalization.\n",
        "For Boosting:\n",
        "Regularization: Boosting algorithms often have regularization parameters (e.g., learning_rate, subsample, colsample_bytree in Gradient Boosting) that can help prevent overfitting. Tune these parameters carefully.\n",
        "Early stopping: Monitor the performance on a validation set during training and stop training when the performance starts to degrade.\n",
        "Limit base model complexity: Use simpler base models (e.g., shallow decision trees or \"stumps\").\n",
        "Step 3: Select Base Models\n",
        "\n",
        "Diversity is Key: Ensemble methods benefit from diverse base models. Consider using different types of models (e.g., decision trees, logistic regression, support vector machines) or the same type of model with different hyperparameters or trained on different subsets of features.\n",
        "Weak Learners: For Boosting, the base models are typically \"weak learners,\" meaning they perform slightly better than random chance. Decision trees (especially shallow ones) are common choices.\n",
        "For Bagging: The base models can be strong learners, as Bagging primarily focuses on reducing variance.\n",
        "Step 4: Evaluate Performance Using Cross-Validation\n",
        "\n",
        "Why Cross-Validation? Cross-validation provides a more robust estimate of the model's performance on unseen data compared to a single train-test split. It helps assess how well the model generalizes.\n",
        "Procedure:\n",
        "Split the data into k folds.\n",
        "For each fold, train the ensemble model on the remaining k-1 folds and evaluate it on the held-out fold.\n",
        "Calculate the performance metric (e.g., accuracy, precision, recall, F1-score, AUC) for each fold.\n",
        "Average the performance metrics across all folds to get an overall estimate of the model's performance.\n",
        "Metrics: Choose appropriate evaluation metrics based on the business problem. For loan default prediction, metrics like precision (minimizing false positives - predicting default when it doesn't happen) and recall (minimizing false negatives - failing to predict default when it does happen) are crucial, in addition to overall accuracy and AUC.\n",
        "Step 5: Justify How Ensemble Learning Improves Decision-Making\n",
        "\n",
        "Increased Accuracy: By combining the predictions of multiple models, ensemble learning can often achieve higher accuracy than any single model. This means fewer misclassifications of loan applications, leading to better risk assessment.\n",
        "Improved Robustness: Ensemble models are generally more robust to noise and outliers in the data because the errors of individual models tend to cancel each other out. This leads to more reliable predictions.\n",
        "Reduced Variance: Bagging specifically helps reduce variance, making the model less sensitive to small changes in the training data. This leads to more stable predictions.\n",
        "Better Generalization: Ensemble methods are less prone to overfitting, which means they generalize better to new, unseen data. This is critical for making accurate predictions on future loan applications.\n",
        "Enhanced Decision Support: By providing more accurate and reliable predictions of loan default, ensemble learning can significantly improve the decision-making process for loan approvals, risk management, and resource allocation within the financial institution. This can lead to reduced financial losses and improved profitability.\n"
      ],
      "metadata": {
        "id": "A8Z5yEysDcH9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwMOQhQ3EMsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}